# -*- coding: utf-8 -*-
"""Копия блокнота "estimate_facial_age (1).ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hx51NHUy8iypwgXH2ckqW1PLXU7afANJ
"""

from google.colab import drive
drive.mount('/content/drive/')
path = r"content/drive/My Drive/face_age-20210502T105747Z-001/face_age/"

"""<a></a>
 # 1. Loading Data
"""

import numpy as np
import pandas as pd

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D,Dropout, Flatten, Dense
from keras import regularizers
from keras.optimizers import SGD
from keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import LabelEncoder # one-hot encoding for age
from keras.utils.np_utils import to_categorical
import keras

import matplotlib.image as mpimg
import matplotlib.pyplot as plt
from PIL import Image
from sklearn.model_selection import train_test_split
import cv2
import os

image_path = '/content/drive/MyDrive/face_age-20210502T105747Z-001/face_age/'

# Take a look at some images

def see_sample_images(age, number_of_images):
    plt.figure(figsize=(10,10))
    age_folder = image_path + age
    images = os.listdir(age_folder)[:number_of_images]
    for i in range(number_of_images):
        file = mpimg.imread(age_folder +'/'+ images[i])
        plt.subplot(number_of_images/2,2,i+1)
        plt.imshow(file)

see_sample_images("001",6)

X = []
Y = []
width = 100
height = 100 #

for folder_name,_,filenames in os.walk(image_path):
    if folder_name !="face_age" and folder_name != 'Data':
        for file in filenames:
            file_path = folder_name +"/"+ file
            image = Image.open(file_path)
            image = image.convert('RGB')
            image = image.resize((width, height))
            X.append(np.array(image))
            Y.append(int(folder_name[-3:]))
    else:
        pass

# Normalise input data to range [0,1]
X = np.array(X)
X = X.astype('float32')
X /= 255.0
# confirm the normalization
print('Min: %.3f, Max: %.3f' % (X.min(), X.max()))

Y = np.array(Y)

os.mkdir("Data")

try:
    os.remove("Data/X.npy")
    os.remove("Data/Y.npy")
    os.remove("Data/X_train.npy")
    os.remove("Data/Y_train.npy")
    os.remove("Data/X_test.npy")
    os.remove("Data/Y_test.npy")
    os.remove("Data/X_val.npy")
    os.remove("Data/Y_val.npy")
except OSError:
    pass

# np.save('Data/X.npy', X)
# np.save('Data/Y.npy',Y)
# X = np.load('Data/X.npy')
# Y = np.load('Daata/Y.npy')

# Train, validation and testing split (70/15/15)
test_size = 0.3
seed = 42
X_train, X_test, Y_train, Y_test = train_test_split(X, 
                                                    Y,
                                                    test_size=test_size, 
                                                    random_state=seed,
                                                    shuffle=True,
                                                    stratify=Y)

X_test, X_val, Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.5)

# save to files
np.save('Data/X_train.npy', X_train)
np.save('Data/Y_train.npy', Y_train)
np.save('Data/X_val.npy', X_val)
np.save('Data/Y_val.npy', Y_val)
np.save('Data/X_test.npy', X_test)
np.save('Data/Y_test.npy', Y_test)

# confirm the saved files
os.listdir("Data")

# check out a random image and its corresponding age label
plt.imshow(X_train[2])

Y_train[2]

"""# 2. Preprocessing"""

# Apply Image Augmentation

train_datagen = ImageDataGenerator(
    shear_range = 0.2, # random application of shearing
    zoom_range = 0.2,
    horizontal_flip = True) # randomly flipping half of the images horizontally

test_datagen = ImageDataGenerator(
)

"""# 3. Callbacks and Default Hyperparameters"""

from keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping
from timeit import default_timer as timer

class TimingCallback(Callback):
    def __init__(self, logs={}):
        self.logs=[]
    def on_epoch_begin(self, epoch, logs={}):
        self.starttime = timer()
    def on_epoch_end(self, epoch, logs={}):
        self.logs.append(timer()-self.starttime)

early_stopping = EarlyStopping(
                                patience=5, # wait for 5 epochs
                                min_delta = 0.01, # if in 5 epochs the loss function doesn't inrease (for accuracy) 
                                               # or decrease (for val_loss) by 1%, then stop
                                verbose=1, # print the training epoch on which training was stopped
                                mode = 'min',
                                monitor='val_loss')

reduce_learning_rate = ReduceLROnPlateau(
                                    monitor="val_loss",
                                    patience=3, # if val_loss plateaus for 3 epochs such that it doesn't see 
                                                # an improvement of size = epsilon
                                    episilon= 0.01,
                                    factor=0.1,  # then we reduce the learning rate by a factor of 0.1
                                    cooldown = 4, # and we wait for 4 epochs before we restart again
                                    verbose=1)

time_callback = TimingCallback()

        
# hyperparameters
lr = 0.01
epochs = 30 # setting it to a low number since this is for Kaggle, ideally you should set this to a higher number ~ 100 so that the model overfits training data, and then apply the elbow methods to select the best params on validaion data
batch_size = 128
results = {}
input_shape =[width,height]
num_of_ages = 100

"""# 4. Simple CNN Model

* A simple CNN model with 6 layers:
Input - Conv - MaxPool - Dropout - Flatten - Output
"""

def baseline_model():
    model = Sequential()
    model.add(Conv2D(filters=32,kernel_size=(3,3),activation="relu",
                   padding="valid",
                   kernel_regularizer=regularizers.l2(0.00001),
                   input_shape=(input_shape[0], input_shape[1], 3)))
    model.add(MaxPooling2D(pool_size=2, strides=2, padding='valid'))
    model.add(Dropout(0.5))
    model.add(Flatten())
    model.add(Dense(1,activation="linear")) 
    return model

Adam= keras.optimizers.Adam(lr=0.1,beta_1=0.9, beta_2=0.999, decay=0.0)

model = baseline_model()

model.compile(optimizer="adam",loss="mean_absolute_error",metrics=['mean_absolute_error'])

model_history = model.fit_generator(
            train_datagen.flow(X_train,Y_train,
                         batch_size = batch_size), # use augmented images
            validation_data = (X_val,Y_val),
            steps_per_epoch=X_train.shape[0] // batch_size,
            epochs = epochs,
            callbacks = [
                         reduce_learning_rate,
                         early_stopping,
                         time_callback
                        ],
            verbose=True)

baseline_adam_train_loss = model_history.history["loss"]
baseline_adam_val_loss = model_history.history["val_loss"]
baseline_adam_train_acc = model_history.history["mean_absolute_error"]
baseline_adam_val_acc = model_history.history["val_mean_absolute_error"]


results["baseline_adam"] = {'train-loss': baseline_adam_train_loss,
                             'val-loss': baseline_adam_val_loss,
                             'train-mae': baseline_adam_train_acc,
                             'val-mae': baseline_adam_val_acc}

"""# 5. Evaluate performance on test set

"""

# Baseline model

model_test_results = model.evaluate(X_test, Y_test, batch_size=128)
print(dict(zip(model.metrics_names, model_test_results)))

